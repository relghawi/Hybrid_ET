"""Hybird multi-task multilayer."""


import xarray as xr
import numpy as np
from sympy import *
from argparse import ArgumentParser
from typing import List, Dict, Tuple
from typing import Optional
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Function
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning import Trainer, seed_everything
from data_utils import Normalize
from Fed3 import MultiLinear
from scipy import stats



class Hybrid(pl.LightningModule):
    def __init__(
            self,
            hparams,
            variables_rs: List[str],
            variables_ra: List[str],
            targets: List[str],
            latent_rs:List[str],
            latent_ra:List[str],
            norm: Normalize,
            ds_val: xr.Dataset,
            ds_test:xr.Dataset,
            ds_train:xr.Dataset,
            hidden_size1,
            hidden_size2,
            n_hidden_layers1,
            n_hidden_layers2,
            dropout
            ) -> None:
       

        super(Hybrid,self).__init__()
        self.save_hyperparameters(
            'hparams',
            'variables_rs',
            'variables_ra',
            'targets',
            'latent_rs',
            'latent_ra',
            'hidden_size1',
            'hidden_size2',
            'n_hidden_layers1',
            'n_hidden_layers2',
            'dropout'
        )
        

        self.hparams.update(hparams) #version 1.5.8
        pl.seed_everything(self.hparams['seed'])

        hidden_size1=self.hparams['hidden_size1']
        n_hidden_layers1=self.hparams['n_hidden_layers1']
        hidden_size2=self.hparams['hidden_size2']
        n_hidden_layers2=self.hparams['n_hidden_layers2']     
        dropout=self.hparams['dropout']        
        self.variables_rs = variables_rs
        self.variables_ra = variables_ra
        self.targets = targets
        self.latent_rs=latent_rs
        self.latent_ra=latent_ra
        self.input_norm_rs = norm.get_normalization_layer(variables=self.variables_rs, invert=False, stack=True)
   
        self.input_norm_ra = norm.get_normalization_layer(variables=self.variables_ra, invert=False, stack=True)

       

        self.nn_rs = MultiLinear(
            num_inputs=len(self.variables_rs),
            num_outputs=len(self.latent_rs),
            num_hidden=hidden_size1,
            num_layers=n_hidden_layers1,
            dropout=dropout,
            dropout_last=False,
            activation=nn.ReLU(),
            activation_last=False
        )
        self.nn_ra = MultiLinear(
            num_inputs=len(self.variables_ra),
            num_outputs=len(self.latent_ra),
            num_hidden=hidden_size2,
            num_layers=n_hidden_layers2,
            dropout=dropout,
            dropout_last=False,
            activation=nn.ReLU(),
            activation_last=False
        )


        self.latent_denorm_rs = norm.get_normalization_layer(variables=self.latent_rs, invert=True, stack=True)

        self.latent_denorm_ra = norm.get_normalization_layer(variables=self.latent_ra, invert=True, stack=True)
        
        self.ds_val = ds_val
        self.ds_test = ds_test
        self.ds_train = ds_train



        # for entries later generated during validation and testing
        self.val_history = np.zeros(100000, dtype=np.float32) * np.nan
        self.test_history = np.zeros(100000, dtype=np.float32) * np.nan
        self.train_history = np.zeros(100000, dtype=np.float32) * np.nan


    def forward(self, batch: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:

        z_rs = self.input_norm_rs({x: batch[x] for x in["VPD","NETRAD_o","WD","TA",'SW_IN', 'SW_POT_sm']})
        z_ra = self.input_norm_ra({x:batch[x] for x in["WS","USTAR"]})

        z_rs = self.nn_rs(z_rs)
        z_ra = self.nn_ra(z_ra)

   
       # denormalization done for latent
        
        Rs_hat = self.latent_denorm_rs(z_rs)
        ra_hat = self.latent_denorm_ra(z_ra)


        Rs_hat = F.softplus(Rs_hat)
        ra_hat = F.softplus(ra_hat)


        Rs_hat=Rs_hat.squeeze(1)
        ra_hat=ra_hat.squeeze(1)

        # Physical PM eqn.
        cp = 1004.834 # specific heat of air for constant pressure (J K-1 kg-1)
        eps= 0.622  # ratio of the molecular weight of water vapor to dry air (=Mw/Md)
        k1 = 2.501
        k2 = 0.00237
        ld = ( k1 - k2 * batch["TA"] ) * 1e+06 #Latent heat of vaporization as a function of air temperature (J kg-1)
        Rd= 287.0586 # gas constant of dry air (J kg-1 K-1) (Foken 2008 p. 245)
        rhoa = (batch["PA"]*1000)/(Rd * (batch["TA"]+273.15)) # Air density(kg m-3)
        psicst  = (cp * batch["PA"]) / (eps * ld) #psychrometric constant (kPa K-1)

        ## Calculating T surface form Stefan-Boltzmann eqn
        emis=0.94
        boltz_cst= 5.789 * 1e-8 # Boltzman constant W/m^2.K^4
        LW_Emis=batch["LW_OUT"]/(boltz_cst*emis)
        TSurf = torch.pow(LW_Emis,(1/4)) ## surface temperature in Kelvin
        TSurf = TSurf- 273.15 ## Ts in celcius
        #print("Tsurf")
        #
        H_hat= ((rhoa * cp) / ra_hat)*(TSurf-batch["TA"])  ## from aerodunamic laws to derive PM

        LE_hat = (batch["delta"] * (batch["NETRAD_o"]-batch["G_o"]) + rhoa * cp * batch["VPD"] / ra_hat) / (( 1 + ( Rs_hat / ra_hat)) * psicst + batch["delta"])
        #LE_hat = (ra_hat*batch["delta"] * (batch["NETRAD_o"]-batch["G_o"]) + rhoa * cp * batch["VPD"]) / (ra_hat*batch["delta"] + psicst*ra_hat+psicst*Rs_hat) ### same mathematically as PM, but may be different numerically for calc of ratio by network 
        #LE_hat = (batch["delta"] * (batch["NETRAD_o"]-batch["G_o"]) + rhoa * cp * batch["VPD"] / ra_hat) / (( 1 + Rs_hat) * psicst + batch["delta"]) 
             
        return Rs_hat,ra_hat,emis,TSurf,LE_hat,H_hat



    def shared_step(self,i): #shared step for train-val-test
    
        time_idx=i["time_idx"] 
        site_idx=i["site_idx"]
        Rs_hat,ra_hat,emis,TSurf,LE_hat,H_hat = self(i)
        return Rs_hat,ra_hat,emis,TSurf,LE_hat,H_hat,time_idx,site_idx
          
    def shared_loss(self,obs,tar,obsH,tarH): #shared ra constarining factor and weight

        LE_loss=F.l1_loss(obs,tar) #MSE loss

        H_loss=F.l1_loss(obsH,tarH)

        total_loss = LE_loss + H_loss

        return LE_loss,H_loss,total_loss

    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int)  -> torch.Tensor: 

        Rs_hat,ra_hat,emis,TSurf,LE_hat,H_hat,time_idx,site_idx=self.shared_step(batch)

        LE_loss,H_loss,total_loss= self.shared_loss(batch['LE_CORR_o'],LE_hat,batch["H_CORR_o"],H_hat)

        train_loss = total_loss
        
        # Logging.
        self.log("train_loss",train_loss,on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.logger.experiment.add_scalars("Performance_Train",{"train_loss": train_loss, "LE_loss":LE_loss, "H_loss":H_loss}, self.global_step) 

        for i, val in enumerate(TSurf):
            self.log('TSurf', val, on_step=True, on_epoch=True, prog_bar=True, logger=True)                  
        return {"loss": train_loss,'Rs_hat':Rs_hat.detach(), 'ra_hat':ra_hat.detach(),'LE_hat': LE_hat.detach(),'H_hat': H_hat.detach(),"TSurf":TSurf.detach(),'time_idx':time_idx.detach(),'site_idx':site_idx.detach()}


    def training_epoch_end(self, training_step_outputs) -> None:

        for var in ["LE_CORR_o","rs","ra","H_CORR_o","TSurf"]:    
            var_new = var +  '_pred'    
            dummy = self.ds_train[var].copy()
            dummy.values[:] = np.nan
            self.ds_train[var_new]= dummy.copy()

        for item in training_step_outputs:
            LE_hat = item['LE_hat'].cpu()
            H_hat = item['H_hat'].cpu()
            Rs_hat = item['Rs_hat'].cpu()
            ra_hat = item['ra_hat'].cpu()
            TSurf = item['TSurf'].cpu()
            
            time_idx = item['time_idx'].cpu()
            site_idx = item['site_idx'].cpu()

            # Assign predictions to the right time steps.
            self.ds_train['LE_CORR_o_pred'].values[time_idx,site_idx] = LE_hat
            self.ds_train['H_CORR_o_pred'].values[time_idx,site_idx] = H_hat
            self.ds_train['rs_pred'].values[time_idx,site_idx] = Rs_hat 
            self.ds_train['ra_pred'].values[time_idx,site_idx] = ra_hat
            self.ds_train['TSurf_pred'].values[time_idx,site_idx] = TSurf


  

    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, torch.Tensor]:

        Rs_hat,ra_hat,emis,TSurf,LE_hat,H_hat,time_idx,site_idx=self.shared_step(batch)

        LE_loss,H_loss,total_loss= self.shared_loss(batch['LE_CORR_o'],LE_hat,batch["H_CORR_o"],H_hat)
        
        val_loss = total_loss
  
        self.log('val_loss',val_loss,prog_bar=True, logger=True)
        self.logger.experiment.add_scalars("Performance_Val",{"val_loss": val_loss, "LE_loss":LE_loss,"H_loss":H_loss}, self.global_step)

        for i, val in enumerate(TSurf):
            self.log('TSurf', val, on_step=True, on_epoch=True, prog_bar=True, logger=True)  
           
    
        return {'Rs_hat':Rs_hat, 'ra_hat':ra_hat,'LE_hat': LE_hat,'H_hat': H_hat,"TSurf":TSurf,'time_idx':time_idx,'site_idx':site_idx}

    def validation_epoch_end(self, validation_step_outputs) -> None:
                
        for var in ["LE_CORR_o","rs","ra","H_CORR_o","TSurf"]:    
            var_new = var +  '_pred'    
            dummy = self.ds_val[var].copy()
            dummy.values[:] = np.nan
            self.ds_val[var_new]= dummy.copy()

        for item in validation_step_outputs:
            LE_hat = item['LE_hat'].cpu()
            H_hat = item['H_hat'].cpu()
            Rs_hat = item['Rs_hat'].cpu()
            ra_hat = item['ra_hat'].cpu()
            TSurf = item['TSurf'].cpu()
            
            time_idx = item['time_idx'].cpu()
            site_idx = item['site_idx'].cpu()

            # Assign predictions to the right time steps.
            self.ds_val['LE_CORR_o_pred'].values[time_idx,site_idx] = LE_hat
            self.ds_val['H_CORR_o_pred'].values[time_idx,site_idx] = H_hat
            self.ds_val['rs_pred'].values[time_idx,site_idx] = Rs_hat 
            self.ds_val['ra_pred'].values[time_idx,site_idx] = ra_hat
            self.ds_val['TSurf_pred'].values[time_idx,site_idx] = TSurf
                           

        return(self.ds_val)
  
            
    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, torch.Tensor]:

        Rs_hat,ra_hat,emis,TSurf,LE_hat,H_hat,time_idx,site_idx=self.shared_step(batch)

        LE_loss,H_loss,total_loss = self.shared_loss(batch['LE_CORR_o'],LE_hat,batch["H_CORR_o"],H_hat)

        test_loss = total_loss


        self.log('test_loss', test_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) 

        self.logger.experiment.add_scalars("Performance_test",{"test_loss": test_loss, "LE_loss":LE_loss,"H_loss":H_loss}, self.global_step)
        
        for i, val in enumerate(TSurf):
            self.log('TSurf', val, on_step=True, on_epoch=True, prog_bar=True, logger=True)     
        return {'Rs_hat':Rs_hat,'ra_hat':ra_hat, 'LE_hat': LE_hat,'H_hat': H_hat,"TSurf":TSurf,'time_idx':time_idx,'site_idx':site_idx}

    def test_epoch_end(self,test_step_outputs) -> None:  

        for var in ["LE_CORR_o","rs","ra","H_CORR_o","TSurf"]:    
            var_new = var +  '_pred'    
            dummy = self.ds_test[var].copy()
            dummy.values[:] = np.nan
            self.ds_test[var_new]= dummy.copy()


        for item in test_step_outputs:
            LE_hat = item['LE_hat'].cpu()
            H_hat = item['H_hat'].cpu()
            Rs_hat = item['Rs_hat'].cpu()
            ra_hat = item['ra_hat'].cpu()
            time_idx = item['time_idx'].cpu()
            site_idx = item['site_idx'].cpu()
            TSurf = item['TSurf'].cpu()

            # Assign predictions to the right time steps.
            self.ds_test['LE_CORR_o_pred'].values[time_idx,site_idx] = LE_hat
            self.ds_test['H_CORR_o_pred'].values[time_idx,site_idx] = H_hat
            self.ds_test['rs_pred'].values[time_idx,site_idx] = Rs_hat     
            self.ds_test['ra_pred'].values[time_idx,site_idx] = ra_hat    
            self.ds_test['TSurf_pred'].values[time_idx,site_idx] = TSurf             


        return(self.ds_test)


    def configure_optimizers(self) -> torch.optim.Optimizer:

        optimizer= torch.optim.AdamW(self.parameters(), lr=self.hparams['learning_rate1'],weight_decay=self.hparams["weight_decay"])

        return optimizer

